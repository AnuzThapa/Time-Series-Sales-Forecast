# -*- coding: utf-8 -*-
"""sales prediction- time series analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kowHJ0EyNK0FC0Rody0k6mXwdwmPN6x_
"""

import pandas as pd
from statsmodels.tsa.statespace.sarimax import SARIMAX
import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# import kaggle
!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download juhi1994/superstore

from zipfile import ZipFile
file_name='superstore.zip'

with ZipFile(file_name,'r') as zip:
  zip.extractall('superstore')

df=pd.read_excel('/content/superstore/US Superstore data.xls')
df.head()

df.columns

df.isna().sum()

df.info()

df.nunique()

# country possess single category so drop it and rowid is of no need
df=df.drop(['Country','Row ID'],axis=1)

df.shape

df.duplicated().sum()   #one row with duplicated value

df=df.drop_duplicates(keep="first")

df=df[['Order Date','Product Name', "Category","Sub-Category","Sales","Quantity","Profit"]]

df.head()

df.describe()

category_counts=df['Category'].value_counts(normalize=True)
category_counts

num_categories=len(category_counts)
color_palette = sns.color_palette(n_colors=num_categories)
sns.barplot(x=category_counts.index,y=category_counts.values,palette=color_palette)
ax = sns.barplot(x=category_counts.index, y=category_counts.values, palette=color_palette)

# Set the title and labels for the plot
ax.set_title('Distribution of Category')
ax.set_xlabel('Category')
ax.set_ylabel('Proportion')

plt.show()

sub_category_count=df['Sub-Category'].value_counts(normalize=True)
sub_category_count

color_palette=sns.color_palette(n_colors=len(sub_category_count))
ax=sns.barplot(x=sub_category_count.index,y=sub_category_count.values,palette=color_palette)
ax.set_title("Sub Category count distribution")
ax.set_xlabel("sub category")
ax.set_ylabel("proportion")
plt.xticks(rotation=45)
plt.show()

# showing profit and sales per category
df_profit_sales=df.groupby("Category")[['Profit','Sales']].agg(['sum'])

df_profit_sales.plot.bar()

plt.figure(figsize=(10,8))
plt.subplot(2,1,1)
sns.distplot(df['Sales'])
plt.title("Sales Distribution")
plt.subplot(2,1,2)
sns.boxplot(df["Sales"])
plt.title("Sales Boxplot")
plt.subplots_adjust(hspace=0.4)
plt.show()

df['Order Date'].value_counts()

earliest_time=min(df['Order Date'])
latest_time=max(df['Order Date'])

print("earliest time :", earliest_time)
print("latest time :",latest_time)

time_diff=df['Order Date'].diff()

time_diff

by_category = df.groupby(['Category','Order Date']).sum("Sales").reset_index()

by_category

sns.set(rc={'figure.figsize':(14,8)})    #after doing this no need to do plt.figure... every time
ax = sns.lineplot(data=by_category, x ='Order Date', y = 'Sales',
                  hue='Category',legend='full')      #hue=Category means each of category will be shown in different color
plt.ylabel('USD')
plt.xlabel('Time')
plt.title('Sales for all Categories')
plt.show()

ax = sns.lineplot(data=by_category[by_category["Category"]=="Office Supplies"], x ='Order Date', y = 'Sales',color="orange")
plt.ylabel('USD')
plt.xlabel('Time')
plt.title(' Sales for Office Supplies')
plt.show()

ax = sns.lineplot(data=by_category[by_category["Category"]=="Furniture"], x ='Order Date', y = 'Sales',color="blue")
plt.ylabel('USD')
plt.xlabel('Time')
plt.title(' Sales for Office Supplies')
plt.show()

ax = sns.lineplot(data=by_category[by_category["Category"]=="Technology"], x ='Order Date', y = 'Sales',color='green')
plt.ylabel('USD')
plt.xlabel('Time')
plt.title(' Sales for Office Supplies')
plt.show()

by_product = df.groupby(['Product Name','Order Date']).sum("Sales").reset_index()    #sum the Sales from the unique combination of Product name and order date
by_product

ax=sns.lineplot(data=by_product[by_product['Product Name']=="netTALK DUO VoIP Telephone Service"],x="Order Date",y="Sales",hue="Product Name")
plt.title("Sales distribution for products")
plt.xlabel("Order Date")
plt.ylabel("Sales")
plt.show()

ax=sns.lineplot(data=by_product[by_product['Product Name']=="Staple envelope"],x="Order Date",y="Sales",hue="Product Name")
plt.title("Sales distribution for products")
plt.xlabel("Order Date")
plt.ylabel("Sales")
plt.show()

# Time series analysis

# Filter Office Supplies and retain only Order Date and Sales
df_new = by_category[by_category["Category"]=="Office Supplies"][['Order Date', 'Sales']]
# Convert Order Date to datetime dtype
df_new['Order Date'] = pd.to_datetime(df_new['Order Date'])
# Group by month
df_new = df_new.groupby(pd.Grouper(key='Order Date', freq='MS')).sum().reset_index()
# Display
df_new.head(5)

time_diff1 = df_new['Order Date'].diff()
time_diff1.value_counts()

sns.set(rc={'figure.figsize':(14,10)})
ax = sns.lineplot(data=df_new, x ='Order Date', y = 'Sales')
plt.ylabel('USD')
plt.xlabel('Time')
plt.xticks(rotation=90)
plt.show()

def generate_ar_process(lags, coefs, length):

    #cast coefs to np array
    coefs = np.array(coefs)

    #initial values
    series = [np.random.normal() for _ in range(lags)]

    for _ in range(length):
        #get previous values of the series, reversed
        prev_vals = series[-lags:][::-1]

        #get new value of time series
        new_val = np.sum(np.array(prev_vals) * coefs) + np.random.normal()

        series.append(new_val)

    return np.array(series)

# stationarity test
from statsmodels.tsa.stattools import adfuller
def perform_adf_test(series):
    result = adfuller(series)
    print('ADF Statistic: %f' % result[0])
    print('p-value: %f' % result[1])

perform_adf_test(df_new['Sales'])

# time series data is not stationery
# using differencing method to convert to stationery
df_new['diff_Sales'] = df_new['Sales'].diff()
# Drop missing values
df_new = df_new.dropna()

df_new.head()

# Test stationarity again for new data
perform_adf_test(df_new['diff_Sales'])

# now the data is stationery:mean and variance is constant over time.
# Plot to see data again
from datetime import datetime
sns.lineplot(data=df_new, x ='Order Date', y = 'diff_Sales')
plt.ylabel('USD')
plt.xlabel('Time')
plt.xticks(rotation=90)
plt.title("Data after differencing")
for year in range(2014,2019):
    plt.axvline(datetime(year,1,1), color='k', linestyle='--', alpha=0.5)
plt.show()

from statsmodels.tsa.seasonal import STL
# Remove freq info
df_new.set_index('Order Date', inplace=True)

result = STL(df_new['diff_Sales'],period=12).fit()   #performing seasonal trend decomposition using loess
# seasonal-repeating patterns
# trend-upward or downward trend line
# residue-random noise

seasonal, trend, resid = result.seasonal, result.trend, result.resid

plt.figure(figsize=(14,12))
plt.subplot(4,1,1)
plt.plot(df_new['diff_Sales'])
plt.title('Original Series', fontsize=16)
plt.subplot(4,1,2)
plt.plot(trend)
plt.title('Trend', fontsize=16)
plt.subplot(4,1,3)
plt.plot(seasonal)
plt.title('Seasonal', fontsize=16)
plt.subplot(4,1,4)
plt.plot(resid)
plt.title('Residual', fontsize=16)
plt.tight_layout()

# anamaoly detection
# Explainable data
estimated = trend + seasonal
#
plt.figure(figsize=(16,8))
plt.plot(df_new['diff_Sales'],label='True data')
plt.plot(estimated,label='Explainable data')
plt.legend()
plt.title('Data with and without anomaly', fontsize=16)
plt.show()

# visuallizing difference that is residue

"""To quantify if residuals are creating outliers in our data, we use the dispersion analysis to check the distribution of residuals around the mean. Here the threshold is 3 Standard Deviation."""

# Calculate mean and sd
resid_mu = resid.mean()
resid_dev = resid.std()
# Assign upper and lower tails (3SD)
lower = resid_mu - 3*resid_dev
upper = resid_mu + 3*resid_dev
# Plot the data
plt.figure(figsize=(14,8))
plt.plot(resid)
plt.title('Anomaly Detection by distribution of residual', fontsize=16)
# Draw a rectangle to capture normal data
plt.fill_between([datetime(2014,1,1), datetime(2018,2,1)], lower, upper, color='g', alpha=0.25, linestyle='--', linewidth=2)

df_new.head()

# Calculate anomaly
# filter out the data frame to include only the row with residual value
anomalies = df_new[(resid < lower) | (resid > upper)]
# resid < lower) | (resid > upper)===returns a Boolean Series with the same index. When you pass that Boolean Series as a mask to df_new, it selects only the matching anomalous rows.

# Plot anomaly
plt.figure(figsize=(14,8))
plt.plot(df_new['diff_Sales'])
for year in range(2014,2019):
    plt.axvline(datetime(year,1,1), color='k', linestyle='--', alpha=0.5)
plt.scatter(anomalies.index, anomalies.diff_Sales, color='r', marker='D')

anomalies

# checking white noise
# there is no pattern in residual so the data is white noise

# ACF â€” Autocorrelation Function
# Measures the correlation between a time series and its lagged versions.

# Lag k means looking at the value k steps before the current time.

# ACF â€” Partial Autocorrelation Function
# Measures the correlation between a time series and its lag, after removing effects of intermediate lags.

from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
# plot_acf====For each lag k, it plots how strongly the series at time t is correlated with the value at tâˆ’k.

plot_acf(df_new.diff_Sales, lags=12);

# plot_pacf=====Shows the direct correlation between the series and a lag, after removing effects of all shorter lags.

plot_pacf(df_new.diff_Sales, lags=12, method = "ols");

from datetime import timedelta

test_end = datetime(2018,1,1)
train_end = datetime(2016,12,1) # Forecast sales for last 12 months
train_data = df_new[:train_end]
test_data = df_new[train_end + timedelta(days=1):test_end]

from tqdm import tqdm_notebook
import statsmodels.api as sm
def optimize_ARIMA(order_list, exog):
    """
        Return dataframe with parameters and corresponding AIC

        order_list - list with (p, d, q) tuples
        exog - the exogenous variable
    """

    results = []

    for order in tqdm_notebook(order_list):
        try:
            model = sm.tsa.ARIMA(exog, order=order).fit()
        except:
            continue

        aic = model.aic
        results.append([order, model.aic])

    result_df = pd.DataFrame(results)
    result_df.columns = ['(p, d, q)', 'AIC']
    #Sort in ascending order, lower AIC is better
    result_df = result_df.sort_values(by='AIC', ascending=True).reset_index(drop=True)

    return result_df

import itertools
ps = range(0, 10, 1)
d = 1
qs = range(0, 10, 1)
# Create a list with all possible combination of parameters
parameters = itertools.product(ps,qs)
parameters_list = list(parameters)
order_list = []
for each in parameters_list:
    each = list(each)
    each.insert(1, d) # insert d in the second position
    each = tuple(each) # convert to tuble
    order_list.append(each)

result_df = optimize_ARIMA(order_list, exog=train_data['Sales'])
result_df

best_model = sm.tsa.ARIMA(train_data['Sales'], order=(2,1,0)).fit()
print(best_model.summary())

df_new['ARIMA'] = best_model.predict(start=train_end, end=test_end)

# plot
plt.figure(figsize=[15,10])
plt.grid(True)
plt.plot(df_new['Sales'],label='True label')
plt.plot(df_new['ARIMA'],label='ARIMA')
plt.legend()
plt.show

# Performance
print('Mean Absolute Percent Error for ARIMA:', round(np.mean(abs((df_new.loc[df_new.index>train_end,'Sales']-df_new.loc[df_new.index>train_end,'ARIMA'])/df_new.loc[df_new.index > train_end,'Sales']))*100,4),'%')

"""p,d,q: Non-seasonal AR order, differencing order, MA order

ð‘ƒ
,
ð·
,
ð‘„
P,D,Q: Seasonal AR order, seasonal differencing order, seasonal MA order
"""

def optimize_SARIMA(parameters_list, d, D, s, exog):
    """
        Return dataframe with parameters, corresponding AIC and SSE

        parameters_list - list with (p, q, P, Q) tuples
        d - integration order
        D - seasonal integration order
        s - length of season
        exog - the exogenous variable
    """

    results = []

    for param in tqdm_notebook(parameters_list):
        try:
            model = SARIMAX(exog, order=(param[0], d, param[1]), seasonal_order=(param[2], D, param[3], s)).fit(disp=-1)
        except:
            continue

        aic = model.aic   # Akaike Information Criterion
        results.append([param, aic])   #it's a key metric used to evaluate and compare statistical models.

    result_df = pd.DataFrame(results)
    result_df.columns = ['(p,q)x(P,Q)', 'AIC']
    #Sort in ascending order, lower AIC is better
    result_df = result_df.sort_values(by='AIC', ascending=True).reset_index(drop=True)

    return result_df

p = range(0, 4, 1)
d = 1
q = range(0, 4, 1)
P = range(0, 4, 1)
D = 1
Q = range(0, 4, 1)
s = 4
parameters = itertools.product(p, q, P, Q)   #gives all the possible combination of p,q,P,Q
parameters_list = list(parameters)   #conversion to list of tuples
print(len(parameters_list))

result_df2 = optimize_SARIMA(parameters_list, d, D, s, train_data['Sales'])
result_df2
# lower AIC is better

best_SARIMA = SARIMAX(train_data['Sales'],order=(1, 1, 2), seasonal_order=(0, 1, 3, 4)).fit()
print(best_SARIMA.summary())

df_new['SARIMAX'] = best_SARIMA.predict(start=train_end, end=test_end)

# plot
plt.figure(figsize=[15,10])
plt.grid(True)
plt.plot(df_new['Sales'],label='True label')
plt.plot(df_new['ARIMA'],label='ARIMA')
plt.plot(df_new['SARIMAX'],label='SARIMAX')
plt.legend()
plt.show

# Performance
print('Mean Absolute Percent Error for SARIMAX:', round(np.mean(abs((df_new.loc[df_new.index>train_end,'Sales']-df_new.loc[df_new.index>train_end,'SARIMAX'])/df_new.loc[df_new.index > train_end,'Sales']))*100,4),'%')

"""# SARIMAX SHOWED BETTER RESULT THEN ARIMAX

# now time series data forecasting with LSTM - the recurrent neural network
"""

import torch.nn as nn
import torch
from torch.utils.data import Dataset, DataLoader

from sklearn.preprocessing import MinMaxScaler
df=df[["Order Date","Sales"]]
sales = df["Sales"].values.reshape(-1, 1)
scaler = MinMaxScaler()
sales_norm = scaler.fit_transform(sales)

df.head()

ax=sns.lineplot(data=df,x="Order Date",y="Sales")
plt.title("Sales distribution for products")
plt.xlabel("Order Date")
plt.ylabel("Sales")
plt.show()

def create_sequences(data, seq_len):
    xs, ys = [], []
    for i in range(len(data) - seq_len):
        xs.append(data[i:i+seq_len])   #from 30 rows of dataset in each sequences, 29 is the input and 30th is the target.each of 29 row is fed the input list and target is fed to the target list.
        ys.append(data[i+seq_len])
    return np.array(xs), np.array(ys)

seq_len = 30
X, y = create_sequences(sales_norm, seq_len)

class SalesDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.from_numpy(X).float()
        self.y = torch.from_numpy(y).float()
    def __len__(self):
        return len(self.X)
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]
split = int(len(X) * 0.8)
train_ds = SalesDataset(X[:split], y[:split])
val_ds = SalesDataset(X[split:], y[split:])
train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)
val_loader = DataLoader(val_ds, batch_size=16)

# wrapping up our dataset to the pytorch dataset

class LSTMForecast(nn.Module):
    def __init__(self, n_features=1, hidden_size=50, num_layers=2):
        super().__init__()
        self.lstm = nn.LSTM(n_features, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, 1)  # A fully connected (linear) layer that maps the LSTM output to a single prediction value.
    def forward(self, x):
        out, _ = self.lstm(x)
        return self.fc(out[:, -1, :])

model = LSTMForecast()
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

for epoch in range(20):
    model.train()
    total_loss = 0
    for xb, yb in train_loader:
        optimizer.zero_grad()
        pred = model(xb)
        loss = criterion(pred, yb)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    model.eval()
    val_loss = np.mean([criterion(model(xb), yb).item() for xb, yb in val_loader])
    print(f"{epoch+1} â†’ train {total_loss:.4f}, val {val_loss:.4f}")

model.eval()
actuals = []
predictions = []

with torch.no_grad():
    for xb, yb in val_loader:
        pred = model(xb)
        actuals.extend(yb.cpu().numpy().flatten())
        predictions.extend(pred.cpu().numpy().flatten())

# Plot actual vs predicted
plt.figure(figsize=(10, 5))
plt.plot(actuals[:30], label='Actual')
plt.plot(predictions[:30], label='Predicted')
plt.title('Actual vs Predicted Sales on Validation Set')
plt.xlabel('Samples')
plt.ylabel('Sales')
plt.legend()
plt.grid(True)
plt.show()

model.eval()
last_seq = sales_norm[-seq_len:]
preds = []

for _ in range(7):
    inp = torch.from_numpy(last_seq.reshape(1, seq_len, 1)).float()   #30 days of data is fed to predict the next outcome.then we can increse the step by one to predict the next day and so on .finally we get the desierd day of sales prediction.
    pred = model(inp).item()
    preds.append(pred)
    last_seq = np.roll(last_seq, -1)
    last_seq[-1] = pred
preds = scaler.inverse_transform(np.array(preds).reshape(-1,1))
print("Next 7-day forecast:", preds.flatten())

import matplotlib.pyplot as plt
import numpy as np

# Assuming preds is a numpy array with shape (7, 1) after inverse transform
preds = preds.flatten()  # flatten to 1D array for plotting

# Create x-axis values (e.g., days 1 to 7)
days = np.arange(1, len(preds) + 1)

plt.figure(figsize=(8, 4))
plt.plot(days, preds, marker='o', linestyle='-', color='b')
plt.title("7-Day Sales Forecast")
plt.xlabel("Day")
plt.ylabel("Sales")
plt.xticks(days)
plt.grid(True)
plt.show()

"""<!--  -->"""